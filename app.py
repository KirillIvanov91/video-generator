from math import e
from fastapi import FastAPI
from pydantic import BaseModel
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
import torch, uuid, os, imageio
from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_synth import TextToVideoSDPipeline
from torchvision.io import write_video
from pathlib import Path


app = FastAPI()

output_dir = Path("output")
output_dir.mkdir(exist_ok=True)


print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å...")
pipe = TextToVideoSDPipeline.from_pretrained(
    "cerspense/zeroscope_v2_XL",
    torch_dtype=torch.float16
    
).to("cuda")
print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()
pipe.to("cuda")

class VideoRequest(BaseModel):
    prompt: str

@app.post("/generate")
async def generate(req: VideoRequest):
    print(f"GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}")
    print(f"–¢–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {next(pipe.parameters()).device}")
    print(f"üé¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {req.prompt}")
    video_tensor = pipe(req.prompt, num_inference_steps=50).videos[0]
    #video_tensor = pipe(req.prompt, num_frames=60).videos[0]  # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–Ω–∑–æ—Ä (T, H, W, C)
    filename = f"{uuid.uuid4().hex}.mp4"
    output_path = output_dir / filename
    imageio.mimsave(output_path, video_tensor, fps=8)
    print(f"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
    #os.makedirs("output", exist_ok=True)
    #path = f"output/{filename}"
    #write_video(path, video_tensor.permute(0, 3, 1, 2), fps=8)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∫–∞–Ω–∞–ª—ã –∏ –∑–∞–¥–∞–µ–º FPS
    return {"video_path": str(output_path)}

@app.get("/")
async def read_root():
    return {"message": "–°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∑–∞–ø—É—â–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç /generate."}





#docker run --gpus all -p 8000:8000 -v ${PWD}:/app zeroscope-server
