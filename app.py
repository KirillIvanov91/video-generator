from math import e
from fastapi import FastAPI
from pydantic import BaseModel
from diffusers
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
import torch, uuid, os
from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_synth import TextToVideoSDPipeline
from torchvision.io import write_video

app = FastAPI()

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å...")
pipe = TextToVideoSDPipeline.from_pretrained(
    "cerspense/zeroscope_v2_XL",
    torch_dtype=torch.float16
    
).pipe.to("cuda")
print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!")

class VideoRequest(BaseModel):
    prompt: str

@app.post("/generate")
async def generate(req: VideoRequest):
    print(f"GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}")
    print(f"–¢–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {next(pipe.parameters()).device}")
    filename = f"{uuid.uuid4().hex}.mp4"
    os.makedirs("output", exist_ok=True)
    print(f"üé¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {req.prompt}")
    video_tensor = pipe(req.prompt, num_frames=60).videos[0]  # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–Ω–∑–æ—Ä (T, H, W, C)
    result = pipe(req.prompt, num_frames=60).videos[0]
    path = f"output/{filename}"
    write_video(path, video_tensor.permute(0, 3, 1, 2), fps=8)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∫–∞–Ω–∞–ª—ã –∏ –∑–∞–¥–∞–µ–º FPS
    result.save(path)
    return {"video_path": path}

@app.get("/")
async def read_root():
    return {"message": "–°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∑–∞–ø—É—â–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç /generate."}





#docker run --gpus all -p 8000:8000 -v ${PWD}:/app zeroscope-server
